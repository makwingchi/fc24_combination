{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ff2153-db74-433d-a6a9-a1c39cfa6ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T09:01:36.784822Z",
     "iopub.status.busy": "2023-12-05T09:01:36.784083Z",
     "iopub.status.idle": "2023-12-05T09:01:39.450761Z",
     "shell.execute_reply": "2023-12-05T09:01:39.449806Z",
     "shell.execute_reply.started": "2023-12-05T09:01:36.784794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d956f1-fd62-4058-a909-ca0b3b3529d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "930dc3a1-4ac3-44f7-8cf1-8977083b0223",
   "metadata": {},
   "source": [
    "# log_prob tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f691925-c811-4b48-89ae-24f6216c7bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:09:01.552728Z",
     "iopub.status.busy": "2023-12-05T08:09:01.552109Z",
     "iopub.status.idle": "2023-12-05T08:09:01.564639Z",
     "shell.execute_reply": "2023-12-05T08:09:01.563862Z",
     "shell.execute_reply.started": "2023-12-05T08:09:01.552701Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = Categorical(logits=torch.tensor([ 0.3, 0.2, 0.25, 0.25 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d9cbe6c-9b05-45a6-b59a-e3a826355506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:22:03.511608Z",
     "iopub.status.busy": "2023-12-05T08:22:03.511009Z",
     "iopub.status.idle": "2023-12-05T08:22:03.522822Z",
     "shell.execute_reply": "2023-12-05T08:22:03.522062Z",
     "shell.execute_reply.started": "2023-12-05T08:22:03.511581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9906, 0.7064, 0.6948, 0.1903, 0.2622])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_logits = torch.rand(5)\n",
    "action_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8e904b6-4028-4e7a-844f-e9749720cd43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:22:50.737640Z",
     "iopub.status.busy": "2023-12-05T08:22:50.736998Z",
     "iopub.status.idle": "2023-12-05T08:22:50.745039Z",
     "shell.execute_reply": "2023-12-05T08:22:50.744158Z",
     "shell.execute_reply.started": "2023-12-05T08:22:50.737613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2917, 0.2195, 0.2170, 0.1310, 0.1408])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs = torch.nn.functional.softmax(action_logits, dim=-1)\n",
    "action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb544386-35dc-4c4a-8cf9-77b7bc4719ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:23:06.407120Z",
     "iopub.status.busy": "2023-12-05T08:23:06.406534Z",
     "iopub.status.idle": "2023-12-05T08:23:06.411764Z",
     "shell.execute_reply": "2023-12-05T08:23:06.410926Z",
     "shell.execute_reply.started": "2023-12-05T08:23:06.407089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = Categorical(action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db68072c-0f5b-49a0-9211-7419fa1ebf97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:23:26.133505Z",
     "iopub.status.busy": "2023-12-05T08:23:26.133168Z",
     "iopub.status.idle": "2023-12-05T08:23:26.139583Z",
     "shell.execute_reply": "2023-12-05T08:23:26.138738Z",
     "shell.execute_reply.started": "2023-12-05T08:23:26.133481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = dist.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab726b37-6bf7-4666-be0f-a01cc9f5475b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:23:36.770350Z",
     "iopub.status.busy": "2023-12-05T08:23:36.769989Z",
     "iopub.status.idle": "2023-12-05T08:23:36.776696Z",
     "shell.execute_reply": "2023-12-05T08:23:36.775853Z",
     "shell.execute_reply.started": "2023-12-05T08:23:36.770323Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.5279)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b979fe4-97eb-43b4-a599-e3e6e41afabf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:23:56.851692Z",
     "iopub.status.busy": "2023-12-05T08:23:56.851314Z",
     "iopub.status.idle": "2023-12-05T08:23:56.859693Z",
     "shell.execute_reply": "2023-12-05T08:23:56.858754Z",
     "shell.execute_reply.started": "2023-12-05T08:23:56.851665Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.5279)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(action_probs[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c6f41-48c7-4ac2-a0f4-27778e987fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd974a7-c39a-4bd8-a7fe-6c40b32e07d5",
   "metadata": {},
   "source": [
    "# REINFORCE Simplified Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88de087-1e25-4b72-b64f-1cc5ecf56b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T09:01:42.342280Z",
     "iopub.status.busy": "2023-12-05T09:01:42.341822Z",
     "iopub.status.idle": "2023-12-05T09:01:42.348669Z",
     "shell.execute_reply": "2023-12-05T09:01:42.347532Z",
     "shell.execute_reply.started": "2023-12-05T09:01:42.342254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9b858d8-84ee-42aa-9046-9d1ce81a29f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T11:37:02.759290Z",
     "iopub.status.busy": "2023-12-05T11:37:02.758638Z",
     "iopub.status.idle": "2023-12-05T11:37:02.770514Z",
     "shell.execute_reply": "2023-12-05T11:37:02.769567Z",
     "shell.execute_reply.started": "2023-12-05T11:37:02.759226Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73d42537-80e3-4b7f-b405-34cca9151767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T11:38:58.290111Z",
     "iopub.status.busy": "2023-12-05T11:38:58.289701Z",
     "iopub.status.idle": "2023-12-05T11:38:58.305981Z",
     "shell.execute_reply": "2023-12-05T11:38:58.304975Z",
     "shell.execute_reply.started": "2023-12-05T11:38:58.290087Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, epochs=50, batch_size=5000, render=False):\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()[0]       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(copy.deepcopy(obs))\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, truncated, info = env.step(act)\n",
    "            \n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    " \n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "                # print(ep_rews)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                # batch_weights += [ep_ret] * ep_len\n",
    "                batch_weights += list(reward_to_go(ep_rews))\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset()[0], False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i+1, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2013a873-7029-4beb-b1ca-f707a401c192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T11:38:58.397649Z",
     "iopub.status.busy": "2023-12-05T11:38:58.397289Z",
     "iopub.status.idle": "2023-12-05T11:39:58.674313Z",
     "shell.execute_reply": "2023-12-05T11:39:58.673739Z",
     "shell.execute_reply.started": "2023-12-05T11:38:58.397626Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 \t loss: 8.966 \t return: 19.909 \t ep_len: 19.909\n",
      "epoch:   2 \t loss: 10.336 \t return: 22.181 \t ep_len: 22.181\n",
      "epoch:   3 \t loss: 11.143 \t return: 24.179 \t ep_len: 24.179\n",
      "epoch:   4 \t loss: 13.012 \t return: 27.582 \t ep_len: 27.582\n",
      "epoch:   5 \t loss: 14.664 \t return: 30.736 \t ep_len: 30.736\n",
      "epoch:   6 \t loss: 16.996 \t return: 36.664 \t ep_len: 36.664\n",
      "epoch:   7 \t loss: 22.514 \t return: 47.245 \t ep_len: 47.245\n",
      "epoch:   8 \t loss: 22.453 \t return: 47.933 \t ep_len: 47.933\n",
      "epoch:   9 \t loss: 22.641 \t return: 55.659 \t ep_len: 55.659\n",
      "epoch:  10 \t loss: 26.092 \t return: 56.444 \t ep_len: 56.444\n",
      "epoch:  11 \t loss: 31.265 \t return: 67.182 \t ep_len: 67.182\n",
      "epoch:  12 \t loss: 28.642 \t return: 68.986 \t ep_len: 68.986\n",
      "epoch:  13 \t loss: 32.890 \t return: 79.873 \t ep_len: 79.873\n",
      "epoch:  14 \t loss: 33.661 \t return: 80.619 \t ep_len: 80.619\n",
      "epoch:  15 \t loss: 44.899 \t return: 119.143 \t ep_len: 119.143\n",
      "epoch:  16 \t loss: 51.157 \t return: 133.395 \t ep_len: 133.395\n",
      "epoch:  17 \t loss: 62.094 \t return: 159.152 \t ep_len: 159.152\n",
      "epoch:  18 \t loss: 84.069 \t return: 193.148 \t ep_len: 193.148\n",
      "epoch:  19 \t loss: 79.762 \t return: 209.625 \t ep_len: 209.625\n",
      "epoch:  20 \t loss: 102.743 \t return: 282.222 \t ep_len: 282.222\n",
      "epoch:  21 \t loss: 110.739 \t return: 314.375 \t ep_len: 314.375\n",
      "epoch:  22 \t loss: 130.890 \t return: 315.562 \t ep_len: 315.562\n",
      "epoch:  23 \t loss: 97.735 \t return: 276.684 \t ep_len: 276.684\n",
      "epoch:  24 \t loss: 97.429 \t return: 314.438 \t ep_len: 314.438\n",
      "epoch:  25 \t loss: 129.864 \t return: 369.429 \t ep_len: 369.429\n",
      "epoch:  26 \t loss: 91.628 \t return: 301.944 \t ep_len: 301.944\n",
      "epoch:  27 \t loss: 155.182 \t return: 476.909 \t ep_len: 476.909\n",
      "epoch:  28 \t loss: 145.343 \t return: 448.667 \t ep_len: 448.667\n",
      "epoch:  29 \t loss: 172.451 \t return: 553.300 \t ep_len: 553.300\n",
      "epoch:  30 \t loss: 267.066 \t return: 850.833 \t ep_len: 850.833\n",
      "epoch:  31 \t loss: 315.650 \t return: 1016.800 \t ep_len: 1016.800\n",
      "epoch:  32 \t loss: 256.531 \t return: 766.286 \t ep_len: 766.286\n",
      "epoch:  33 \t loss: 451.814 \t return: 1317.000 \t ep_len: 1317.000\n",
      "epoch:  34 \t loss: 538.594 \t return: 1034.400 \t ep_len: 1034.400\n",
      "epoch:  35 \t loss: 435.911 \t return: 1058.000 \t ep_len: 1058.000\n",
      "epoch:  36 \t loss: 216.348 \t return: 605.000 \t ep_len: 605.000\n",
      "epoch:  37 \t loss: 343.693 \t return: 1003.800 \t ep_len: 1003.800\n",
      "epoch:  38 \t loss: 308.647 \t return: 665.375 \t ep_len: 665.375\n",
      "epoch:  39 \t loss: 318.247 \t return: 745.714 \t ep_len: 745.714\n",
      "epoch:  40 \t loss: 389.070 \t return: 943.667 \t ep_len: 943.667\n",
      "epoch:  41 \t loss: 413.868 \t return: 812.714 \t ep_len: 812.714\n",
      "epoch:  42 \t loss: 250.231 \t return: 715.143 \t ep_len: 715.143\n",
      "epoch:  43 \t loss: 321.855 \t return: 752.000 \t ep_len: 752.000\n",
      "epoch:  44 \t loss: 605.193 \t return: 1066.400 \t ep_len: 1066.400\n",
      "epoch:  45 \t loss: 766.591 \t return: 1770.800 \t ep_len: 1770.800\n",
      "epoch:  46 \t loss: 380.055 \t return: 1010.400 \t ep_len: 1010.400\n",
      "epoch:  47 \t loss: 658.564 \t return: 1695.750 \t ep_len: 1695.750\n",
      "epoch:  48 \t loss: 714.263 \t return: 2556.500 \t ep_len: 2556.500\n",
      "epoch:  49 \t loss: 409.630 \t return: 1021.200 \t ep_len: 1021.200\n",
      "epoch:  50 \t loss: 955.312 \t return: 2462.333 \t ep_len: 2462.333\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7adcd9-d034-45d1-a2e7-822218ebeb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d71d45-0421-404f-88f5-a7e51576cecf",
   "metadata": {},
   "source": [
    "# Spinningup Exercise 1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "764d135d-eea5-4d74-b397-df27880eefd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T07:06:53.816463Z",
     "iopub.status.busy": "2023-12-06T07:06:53.816084Z",
     "iopub.status.idle": "2023-12-06T07:06:53.821319Z",
     "shell.execute_reply": "2023-12-06T07:06:53.820424Z",
     "shell.execute_reply.started": "2023-12-06T07:06:53.816436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def soln_gaussian_likelihood(x, mu, log_std):\n",
    "    EPS=1e-8\n",
    "    \n",
    "    pre_sum = -0.5 * (((x-mu)/(torch.exp(log_std)+EPS))**2 + 2*log_std + np.log(2*np.pi))\n",
    "    return pre_sum.sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd8bf59b-408d-4060-a51f-c57382495e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T07:03:11.809782Z",
     "iopub.status.busy": "2023-12-06T07:03:11.809165Z",
     "iopub.status.idle": "2023-12-06T07:03:11.814953Z",
     "shell.execute_reply": "2023-12-06T07:03:11.814031Z",
     "shell.execute_reply.started": "2023-12-06T07:03:11.809757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gaussian_likelihood(x, mu, log_std):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Tensor with shape [batch, dim]\n",
    "        mu: Tensor with shape [batch, dim]\n",
    "        log_std: Tensor with shape [batch, dim] or [dim]\n",
    "\n",
    "    Returns:\n",
    "        Tensor with shape [batch]\n",
    "    \"\"\"\n",
    "    dim = x.shape[1]\n",
    "    \n",
    "    first_term = torch.sum(torch.square(x - mu) / torch.square(torch.exp(log_std)) + 2 * log_std, dim=-1)\n",
    "    second_term = dim * torch.log(torch.as_tensor([torch.pi * 2]))\n",
    "    \n",
    "    return -0.5 * (first_term + second_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7746f542-3dab-4f54-a6f5-235537535966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T07:06:55.842148Z",
     "iopub.status.busy": "2023-12-06T07:06:55.841672Z",
     "iopub.status.idle": "2023-12-06T07:06:55.855087Z",
     "shell.execute_reply": "2023-12-06T07:06:55.854124Z",
     "shell.execute_reply.started": "2023-12-06T07:06:55.842125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dim = 10\n",
    "\n",
    "x = torch.rand(batch_size, dim)\n",
    "mu = torch.rand(batch_size, dim)\n",
    "log_std = torch.rand(dim)\n",
    "\n",
    "your_gaussian_likelihood = gaussian_likelihood(x, mu, log_std)\n",
    "true_gaussian_likelihood = soln_gaussian_likelihood(x, mu, log_std)\n",
    "\n",
    "your_result = your_gaussian_likelihood.detach().numpy()\n",
    "true_result = true_gaussian_likelihood.detach().numpy()\n",
    "\n",
    "correct = np.allclose(your_result, true_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "492c5879-21e9-4cf7-a48a-75975a0fd7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T07:06:58.572732Z",
     "iopub.status.busy": "2023-12-06T07:06:58.572244Z",
     "iopub.status.idle": "2023-12-06T07:06:58.578433Z",
     "shell.execute_reply": "2023-12-06T07:06:58.577299Z",
     "shell.execute_reply.started": "2023-12-06T07:06:58.572707Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76d487-b5e3-409a-9ca1-e8adfb238a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db1e07-4716-4c2b-ae65-a4536a96ebc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfd2cfdc-c7f6-4eba-9792-64774ddb4123",
   "metadata": {},
   "source": [
    "# SARSA (Tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3a551d6c-298a-4f3d-a2bb-1a9f50e176d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T11:44:53.049399Z",
     "iopub.status.busy": "2023-12-11T11:44:53.048621Z",
     "iopub.status.idle": "2023-12-11T11:44:53.052996Z",
     "shell.execute_reply": "2023-12-11T11:44:53.052072Z",
     "shell.execute_reply.started": "2023-12-11T11:44:53.049373Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "66faca71-15b2-423d-87a7-4bc34861da48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:17:54.873994Z",
     "iopub.status.busy": "2023-12-12T03:17:54.873371Z",
     "iopub.status.idle": "2023-12-12T03:17:54.883620Z",
     "shell.execute_reply": "2023-12-12T03:17:54.882795Z",
     "shell.execute_reply.started": "2023-12-12T03:17:54.873966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,坐标轴原点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "554c30f0-f1c9-4015-89fb-16eb860cdd00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:17:56.482551Z",
     "iopub.status.busy": "2023-12-12T03:17:56.482220Z",
     "iopub.status.idle": "2023-12-12T03:17:56.491037Z",
     "shell.execute_reply": "2023-12-12T03:17:56.490208Z",
     "shell.execute_reply.started": "2023-12-12T03:17:56.482529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_action)\n",
    "        \n",
    "        return np.argmax(self.Q_table[state])\n",
    "    \n",
    "    def best_action(self, state):\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]\n",
    "        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1\n",
    "        return a\n",
    "    \n",
    "    def update(self, s0, a0, r, s1, a1):\n",
    "        td_err = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b28751b1-296c-431d-972d-dd3e56202662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:17:57.303446Z",
     "iopub.status.busy": "2023-12-12T03:17:57.303022Z",
     "iopub.status.idle": "2023-12-12T03:17:57.308195Z",
     "shell.execute_reply": "2023-12-12T03:17:57.307392Z",
     "shell.execute_reply.started": "2023-12-12T03:17:57.303419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncol = 12\n",
    "nrow = 4\n",
    "env = CliffWalkingEnv(ncol, nrow)\n",
    "np.random.seed(0)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "agent = Sarsa(ncol, nrow, epsilon, alpha, gamma)\n",
    "num_episodes = 500  # 智能体在环境中运行的序列的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "22037643-44fb-46bc-896c-dfa8388406ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:17:57.885795Z",
     "iopub.status.busy": "2023-12-12T03:17:57.885446Z",
     "iopub.status.idle": "2023-12-12T03:17:58.031207Z",
     "shell.execute_reply": "2023-12-12T03:17:58.030588Z",
     "shell.execute_reply.started": "2023-12-12T03:17:57.885771Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_list = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    action = agent.take_action(state)\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_action = agent.take_action(next_state)\n",
    "        episode_return += reward\n",
    "        agent.update(state, action, reward, next_state, next_action)\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    \n",
    "    return_list.append(episode_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48924a5f-94cc-4e02-8d21-92588ba28c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981f6276-f0e2-42e2-a075-cedd4c345150",
   "metadata": {},
   "source": [
    "# Multi-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fb9827a3-c1cc-41af-a0b5-d96336beb7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:21:15.459496Z",
     "iopub.status.busy": "2023-12-12T03:21:15.458971Z",
     "iopub.status.idle": "2023-12-12T03:21:15.476545Z",
     "shell.execute_reply": "2023-12-12T03:21:15.475715Z",
     "shell.execute_reply.started": "2023-12-12T03:21:15.459470Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Nstep_Sarsa:\n",
    "    def __init__(self, n, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "        self.n = n\n",
    "        \n",
    "        self.state_list = []\n",
    "        self.action_list = []\n",
    "        self.reward_list = []\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_action)\n",
    "        \n",
    "        return np.argmax(self.Q_table[state])\n",
    "    \n",
    "    def best_action(self, state):\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]\n",
    "        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1\n",
    "        return a\n",
    "    \n",
    "    def update(self, s0, a0, r, s1, a1, done):\n",
    "        self.state_list.append(s0)\n",
    "        self.action_list.append(a0)\n",
    "        self.reward_list.append(r)\n",
    "        \n",
    "        if len(self.state_list) == self.n:\n",
    "            G = self.Q_table[s1, a1]\n",
    "            \n",
    "            for i in reversed(range(self.n)):\n",
    "                G = self.gamma * G + self.reward_list[i]\n",
    "                \n",
    "                if done and i > 0:\n",
    "                    s = self.state_list[i]\n",
    "                    a = self.action_list[i]\n",
    "                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n",
    "            \n",
    "            s = self.state_list.pop(0)\n",
    "            a = self.action_list.pop(0)\n",
    "            self.reward_list.pop(0)\n",
    "            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n",
    "        \n",
    "        if done:\n",
    "            self.state_list = []\n",
    "            self.action_list = []\n",
    "            self.reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8b970455-ec8c-43ff-9f47-59103d6b8709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:21:15.672482Z",
     "iopub.status.busy": "2023-12-12T03:21:15.672128Z",
     "iopub.status.idle": "2023-12-12T03:21:15.677438Z",
     "shell.execute_reply": "2023-12-12T03:21:15.676523Z",
     "shell.execute_reply.started": "2023-12-12T03:21:15.672456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncol = 12\n",
    "nrow = 4\n",
    "n_step = 5\n",
    "env = CliffWalkingEnv(ncol, nrow)\n",
    "np.random.seed(0)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "agent = Nstep_Sarsa(n_step, ncol, nrow, epsilon, alpha, gamma)\n",
    "num_episodes = 500  # 智能体在环境中运行的序列的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "eeee26f9-486d-4932-972d-5f89b82a53f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:21:15.899419Z",
     "iopub.status.busy": "2023-12-12T03:21:15.899087Z",
     "iopub.status.idle": "2023-12-12T03:21:16.042871Z",
     "shell.execute_reply": "2023-12-12T03:21:16.042060Z",
     "shell.execute_reply.started": "2023-12-12T03:21:15.899397Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_list = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    action = agent.take_action(state)\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_action = agent.take_action(next_state)\n",
    "        episode_return += reward\n",
    "        agent.update(state, action, reward, next_state, next_action, done)\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    \n",
    "    return_list.append(episode_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bd713-b73b-4d7e-97a7-7bf676a0290b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffd3f333-4082-4e16-9d50-b7b05f5452d2",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "23b58322-c875-49c9-b592-bbc55493bdcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:47:02.740215Z",
     "iopub.status.busy": "2023-12-12T03:47:02.739625Z",
     "iopub.status.idle": "2023-12-12T03:47:02.748266Z",
     "shell.execute_reply": "2023-12-12T03:47:02.747394Z",
     "shell.execute_reply.started": "2023-12-12T03:47:02.740190Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Q_Learning:\n",
    "    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_action)\n",
    "        \n",
    "        return np.argmax(self.Q_table[state])\n",
    "    \n",
    "    def best_action(self, state):\n",
    "        Q_max = np.max(self.Q_table[state])\n",
    "        a = [0 for _ in range(self.n_action)]\n",
    "        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n",
    "            if self.Q_table[state, i] == Q_max:\n",
    "                a[i] = 1\n",
    "        return a\n",
    "    \n",
    "    def update(self, s0, a0, r, s1):\n",
    "        td_err = r + self.gamma * np.max(self.Q_table[s1]) - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0e62773c-7ce6-4448-a6a2-b98c8aa1e05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:48:10.581437Z",
     "iopub.status.busy": "2023-12-12T03:48:10.580744Z",
     "iopub.status.idle": "2023-12-12T03:48:10.586365Z",
     "shell.execute_reply": "2023-12-12T03:48:10.585352Z",
     "shell.execute_reply.started": "2023-12-12T03:48:10.581409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncol = 12\n",
    "nrow = 4\n",
    "n_step = 5\n",
    "env = CliffWalkingEnv(ncol, nrow)\n",
    "np.random.seed(0)\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "agent = Q_Learning(ncol, nrow, epsilon, alpha, gamma)\n",
    "num_episodes = 500  # 智能体在环境中运行的序列的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "631403ef-ad9e-4c1d-9536-fcbf9aa5885d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T03:48:22.932603Z",
     "iopub.status.busy": "2023-12-12T03:48:22.932226Z",
     "iopub.status.idle": "2023-12-12T03:48:23.172617Z",
     "shell.execute_reply": "2023-12-12T03:48:23.171205Z",
     "shell.execute_reply.started": "2023-12-12T03:48:22.932577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_list = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    action = agent.take_action(state)\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_action = agent.take_action(next_state)\n",
    "        episode_return += reward\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    \n",
    "    return_list.append(episode_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5500a52-b178-4376-90bf-c278bbcf54bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7163b120-ba6e-429d-9d48-3cb3614fcac8",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f3174-1567-4ed7-85ab-5f0edc50fa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea5365-7172-4d93-9368-052a26b3e603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b25722-bda9-4b81-bf1d-a35198cf38fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
